--- 
title: "Theory-driven analysis of ecological data: a practical handbook"
author: "us"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a draft for our course on model-data.
  The HTML output format for this example is bookdown::gitbook,
  set in the _output.yml file.
link-citations: yes
github-repo: rstudio/bookdown-demo
---

# About Bookdown

This is a _sample_ book written in **Markdown**. You can use anything that Pandoc's Markdown supports; for example, a math equation $a^2 + b^2 = c^2$.

## Usage 

Each **bookdown** chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter *must* start with a first-level heading: `# A good chapter`, and can contain one (and only one) first-level heading.

Use second-level and higher headings within chapters like: `## A short section` or `### An even shorter section`.

The `index.Rmd` file is required, and is also your first book chapter. It will be the homepage when you render the book.

## Render book

You can render the HTML version of this example book without changing anything:

1. Find the **Build** pane in the RStudio IDE, and

1. Click on **Build Book**, then select your output format, or select "All formats" if you'd like to use multiple formats from the same book source files.

Or build the book from the R console:

```{r, eval=FALSE}
bookdown::render_book()
```

To render this example to PDF as a `bookdown::pdf_book`, you'll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.org/tinytex/>.

## Preview book

As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in "Preview book", or from the R console:

```{r eval=FALSE}
bookdown::serve_book()
```


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

## Here are some useful things for writing the book using bookdown 

All chapters start with a first-level heading followed by your chapter title, like the line above. There should be only one first-level heading (`#`) per .Rmd file.

### A section

All chapter sections start with a second-level (`##`) or higher heading followed by your section title, like the sections above and below here. You can have as many as you want within a chapter.

#### An unnumbered section {-}

Chapters and sections are numbered by default. To un-number a heading, add a `{.unnumbered}` or the shorter `{-}` at the end of the heading, like in this section.

## Cross-references {#cross}

Cross-references make it easier for your readers to find and link to elements in your book.

### Chapters and sub-chapters

There are two steps to cross-reference any heading:

1. Label the heading: `# Hello world {#nice-label}`. 
    - Leave the label off if you like the automated heading generated based on your heading title: for example, `# Hello world` = `# Hello world {#hello-world}`.
    - To label an un-numbered heading, use: `# Hello world {-#nice-label}` or `{# Hello world .unnumbered}`.

1. Next, reference the labeled heading anywhere in the text using `\@ref(nice-label)`; for example, please see Chapter \@ref(cross). 
    - If you prefer text as the link instead of a numbered reference use: [any text you want can go here](#cross).

### Captioned figures and tables

Figures and tables *with captions* can also be cross-referenced from elsewhere in your book using `\@ref(fig:chunk-label)` and `\@ref(tab:chunk-label)`, respectively.

See Figure \@ref(fig:nice-fig).

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center', fig.alt='Plot with connected points showing that vapor pressure of mercury increases exponentially as temperature increases.'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Don't miss Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(pressure, 10), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

## Parts

You can add parts to organize one or more book chapters together. Parts can be inserted at the top of an .Rmd file, before the first-level chapter heading in that same file. 

Add a numbered part: `# (PART) Act one {-}` (followed by `# A chapter`)

Add an unnumbered part: `# (PART\*) Act one {-}` (followed by `# A chapter`)

Add an appendix as a special kind of un-numbered part: `# (APPENDIX) Other stuff {-}` (followed by `# A chapter`). Chapters in an appendix are prepended with letters instead of numbers.

## Footnotes and citations 

### Footnotes

Footnotes are put inside the square brackets after a caret `^[]`. Like this one ^[This is a footnote.]. 

### Citations

Reference items in your bibliography file(s) using `@key`.

For example, we are using the **bookdown** package [@R-bookdown] (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and **knitr** [@xie2015] (this citation was added manually in an external file book.bib). 
Note that the `.bib` files need to be listed in the index.Rmd with the YAML `bibliography` key.


The RStudio Visual Markdown Editor can also make it easier to insert citations: <https://rstudio.github.io/visual-markdown-editing/#/citations>

## Blocks

### Equations

Here is an equation.

\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation} 

You may refer to using `\@ref(eq:binom)`, like see Equation \@ref(eq:binom).


### Theorems and proofs

Labeled theorems can be referenced in text using `\@ref(thm:tri)`, for example, check out this smart theorem \@ref(thm:tri).

::: {.theorem #tri}
For a right triangle, if $c$ denotes the *length* of the hypotenuse
and $a$ and $b$ denote the lengths of the **other** two sides, we have
$$a^2 + b^2 = c^2$$
:::

Read more here <https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html>.

### Callout blocks


The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html

## Sharing your book

### Publishing

HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html

### 404 pages

By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you'd like to customize your 404 page instead of using the default, you may add either a `_404.Rmd` or `_404.md` file to your project root and use code and/or Markdown syntax.

### Metadata for sharing

Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the `index.Rmd` YAML. To setup, set the `url` for your book and the path to your `cover-image` file. Your book's `title` and `description` are also used.


This `gitbook` uses the same social sharing data across all chapters in your book- all links shared will look the same.

Specify your book's source repository on GitHub using the `edit` key under the configuration options in the `_output.yml` file, which allows users to suggest an edit by linking to a chapter's source file. 

Read more about the features of this output format here:

https://pkgs.rstudio.com/bookdown/reference/gitbook.html

Or use:

```{r eval=FALSE}
?bookdown::gitbook
```


`r if (knitr::is_html_output()) '
## References {-}
'`


<!--chapter:end:index.Rmd-->

# Preambule

Who is the textbook for?

<!--chapter:end:01-preambule.Rmd-->

# Why mathematical models? (Sonia)


To read:
Deep Symbolic Regression for Physics Guided by Units Constraints: Toward the Automated Discovery of Physical Laws
<https://iopscience.iop.org/article/10.3847/1538-4357/ad014>

Une intelligence artificielle retrouve des lois physiques à partir de données scientifiques
<https://www.insu.cnrs.fr/fr/cnrsinfo/une-intelligence-artificielle-retrouve-des-lois-physiques-partir-de-donnees-scientifiques#:~:text=La%20m%C3%A9thode%20d'intelligence%20artificielle,ces%20r%C3%A8gles%20d'analyse%20dimensionnelle>



## Big data

According to IBM, every day humanity generates 2.5 trillion (2.5 billion billion) bytes of text, image and sound data (<https://www-01.ibm.com/software/fr/data/bigdata>). Acceleration of information accumulation. We can learn from massive amount of data available.

When read by machines (analyses, they can reveal a wealth of unsuspected correlations (or that are difficult to identify). Success of Amazon or Netflix at learning what you like from studying your habits. 

Machine learning (ML), a subset of AI that enables computers to learn from training data, has been highly effective at predicting various types of cancer, including breast, brain, lung, liver, and prostate cancer. In fact, AI and ML have demonstrated greater accuracy in predicting cancer than clinicians.

So, knowledge can emerge from this data analysis (even without any idea of the underlying laws). 

This lead some people to suggest that we may not need theory anymore ("The end of theory: the data deluge, Wired). The availability of data and the development of methods to analyse them: does they mean that we're going through a major epistemological change? In other terms: do we still need models and theory?

<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2711825/>

##	What is theory? 

We define *ecological theory* broadly as an explanation of an ecological phenomenon. These explanations take the form of narratives that explain how an ecological process works, or why an ecological pattern is observed, and that becomes scientifically useful when expressed in a logical structure (Pickett et al. 2010; Rossberg et al. 2019). 

Maris et al. 2017: "Scientific theories contain universal or general propositions regarding the system in question; they generally encompass a set of models, or rules to build models, as representations of the properties and dynamics of target systems. Models can then be expressed as hypotheses within a formal framework (van Fraassen 1980, Giere 2004)."

The transformation of an idea in narrative form into a logical, testable theory often, though not always, involves the use of *models* (Otto and Rosales 2020). 

## What’s a model? 

<https://en.wikipedia.org/wiki/Ecosystem_model>

The concept of a model is derived from scale models, referring to simplified replicas of larger structures such as, for instance, buildings or ships. This implies a structural similarity between a model and its original. Thus, **models are idealized representations of certain aspects of our study systems. They are idealized versions of reality**, just as architectural models represent key features of complex structures and model organisms represent a group of organisms that share common attributes (Kokko 2007). 

Models can be as simple as a verbal statement about a subject or two boxes connected by an arrow to represent some relationship (conceptual models). Alternatively, models can be extremely complex and detailed, such as a mathematical description of the pathways of nitrogen transformations within ecosystems.

A mathematical model can be an equation or a set of equations (mathematical expressions) that describes how different aspects of a system relate to one another (Otto and Day 2007). They can be phenomenological or mechanistic (Grainger et al. 2022). 

Theory and math are not inexorably linked. Indeed, many excellent theories do not involve math (e.g., the theory of evolution by natural selection; Darwin 1859), and many uses of math in ecology are not theory (e.g., practical applications of statistics).

Because models are an idealized, simplified version of the real world (e.g. maps), they are therefore not real. They’re incomplete, they’re wrong (intro Kokko’s book). 
**“all models are false but some models are useful”**
<https://en.wikipedia.org/wiki/All_models_are_wrong>

The systems we’re interested in are complex. How can we hope capturing the complexity of reality? We can’t. 
And if we did, it would not be helpful (if models are as complex as reality, e.g. map) because of: 

- too many parameters to measure (would require too much time and resources),
- the equations are insoluble and require too long to run on even a good computer,
- even if soluble, we might not be able to understand the results. 

**Modeling involves a choice about what to include and what to leave out.**
We need to simplify systems in a way that preserves the essential features of the system (depending on our interest). This means that our big task as modelers is to decide what goes in the model and what doesn’t. The art of modeling is to decide which aspect of reality one can sacrifice and which ones are crucial to retain. 

We want to work with manageable models which max generality, realism and precision towards the goals of understanding and predicting…. 

These goals typically compete with each other so real models are mathematical descriptions that result from tradeoffs among these goals which depend on our needs: the tension between realism, generality, and precision:
**Levins 1966**
<https://v4.chriskrycho.com/2016/realism-generality-and-precision-in-tension.html>

- One can sacrifice generality to realism and precision. E.g. fisheries; good measurement of shot-time behavior, numerical solution, precise testable predictions applicable to a particular situation).

- One can sacrifice realism to generality and precision. E.g. physics-like models (general equations such as LV). The way in which nature deviates from theory will indicate where further complexity will be useful. 

- One can sacrifice precision to realism and generality. E.g. MacArthur 1965. People concerned with the qualitative behavior (not quantitative). Graphical models. 


## Purpose of science?

Understand (and predict). 


## Can we do that with big data analysis?

Big data analysis reveals correlations. (Again, this can be useful)
But those correlations are not causal. 
Famous examples of spurious correlations.
Correlations can emerge by chance because you look at some many variables. 
Correlations can happen because of confounding factors, e.g. the example of shoe size and level of math in a school (due to age)

So we can find correlations, but we often don't know why. So we can't understand (understanding is finding associations that are causal). 

This does not mean that non-causal correlations can't help predict! They can have great predictive power. Amazon, Netflix. 
So we can learn from those data and their analysis, even without causal knowledge. 

We can predict, but we can also fail at predicting. Predictions can fail because lacks of general rules, so extrapolations can fail; financial crisis, elections. It relies on what happened in the past, so unable to work in new situations. 

Predict without understanding.... act without understanding? Risks (cf justice, police, discriminations)

Big data don't create theory, they need it to be exploited. (This last statement could evolve in the future? In particular, new analyses methods suggest they can successfully retrive causality from correlations; cg Zach, Stan's paper with Correlation Cross mapping). 

Because of that, we can argue that we still do need theory and models! (models are one langage of theory, one way of expressing it)

## The scientific method

Let's step back and reflect on the process by which science is carried out. The *scientific method* is an empirical method for acquiring knowledge that has characterized the development of science since at least the 17th century.

Science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time.

-	**'Problem' identification**: It involves careful observation which leads to the formulation of a question.

-	**Hypothesis**: A hypothesis is a conjecture (hypothetical explanations), based on the observation/the knowledge obtained while formulating the question, that may explain any given behavior. A scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment or observation that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested. 
Falsifiability is a deductive standard of evaluation of scientific theories and hypotheses, introduced by the philosopher of science Karl Popper in his book The Logic of Scientific Discovery (1934). 
A theory or hypothesis is falsifiable (or refutable) if it can be logically contradicted by an empirical test.

-	**Prediction**: The prediction step deduces the logical consequences of the hypothesis before the outcome is known. 

-	**Testing**: Hypotheses are tested by conducting experiments or gathering observations. The purpose of the test is to determine whether observations agree with or conflict with the expectations deduced from a hypothesis. 

-	**Refinement (or elimination) of the hypotheses** based on the experimental findings. 

Although procedures vary from one field of inquiry to another, the underlying process is frequently the same. In sum, the process is as follows: making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct. 

**The scientific method is an iterative, cyclical process through which information is continually revised.**

## A feedback loop involving data and models

A scientific understanding of the biological world arises when ideas about how nature works are formalized, tested, refined, and then tested again.

**Scientific inquiry should operate as a feedback loop** in which theory that describes the natural world is developed, tested empirically through carefully articulated hypotheses, modified to better represent reality, and then tested again. 

When this feedback loop works, **theory** provides a framework to guide inquiry, experimental design, and the interpretation of observed patterns, supplies mathematical tools to harness information from collected data, and connects individual experiments to general ideas about how nature operates.

In turn, **empirical research** can be used to support, refute, or revise theoretical predictions, indicate which theoretical assumptions are consistent with the natural world, and point theoreticians to overlooked processes that can be integrated into models.

**However, there is currently a disconnection between theoretical and empirical research.**

Although the benefits of feedback between theoretical and empirical research are widely acknowledged by ecologists, this link is still not as strong as it could be in ecological research. 

Indeed, up to 45% of articles on empirical ecology make no mention of any theory whatsoever (Scheiner 2013), and fewer than 10% of ecologists and evolutionary biologists agree with the statement that 'theoretical findings drive empirical work' in their fields (Haller 2014).

Further references about the disconnect between empirical and theoretical work in ecology: Lomnicki 1988; Kareiva 1989; Fawcett and Higginson 2012; Scheiner 2013; Haller 2014; Rossberg et al. 2019.

**Why this disconnection?**

- A lack of theoretical training in ecology (Rossberg et al. 2019). 
- A lack of motivation on the part of some theoreticians to engage with the language of empiricists (Grimm 1994) or with the elements of nature that empiricists focus on (Krebs 1988).
- A general lack of mutual appreciation between empiricists and theoreticians (Haller 2014). 
- Persistent communication barriers between these two groups (Servedio 2020), in particular that theory is expressed in the langage of math (some ecologists may not have formal background in math and theoretical papers may not be written with a general audience, not always explaining asumptions, terminology and notations), some aspects of the theory may seem inaccessible by some, the more equations an ecology and evolutionary biology article contains, the fewer citations it receives (Fawcett and Higginson 2012).

This barrier presents a major challenge to the full integration of theoretical and empirical work in ecology.

**A better integration of theory into empirical work is needed** (Caswell 1988; Pickett et al. 2010; Marquet et al. 2014; Servedio et al. 2014; Servedio 2020).

This is especially important in the context of global change. 

This is why this course/book! 







<!--chapter:end:02-why-models.Rmd-->

# What types of theoretical models in ecology? (Isa)

## What system? What question? What hypotheses (no maths shown here)

* Question is related to a **specific level of organization** / scale and determine what can be ignored
* Examples of questions and processes of interest for each scale based on our recurrent example plant-herbivore (P-H)
* **Updated Levin's triangle** (Bullock 2014) illustrated by different (P-H) models (conceptual, system-specific, generic) with related questions => define variables
* **What can be ignored?** The differences in time scale between processes allow to do some approximations: Upper level is slower than lower ones and its dynamics may be ignored depending on the question. Similarly, lower levels can be considered at equilibrium if much faster than the focal dynamics, or averaged (macroscale averaged view of microscale processes => physics statistics).
* **Link between assumptions, model and predictions**, with the risk of overinterpretation: always keep in mind that what mathematical methods is truth only in the context of initial assumptions.
* **The different types of assumptions** (Servedio 2014):
  - Critical assumptions: the ones necessary to answer the question (structure of the model).
  - Exploratory assumptions: the ones to go deeper in the exploration, assess the robustness of the results.
  - Logistical assumptions: the ones that have to deal with tractability.
* **Illustration of assumption types** with the Grazing-optimisation example (De mazancourt et al. 1998):
  - Critical assumptions: two paths of recycling with different rates for plants and herbivores.
  - Exploratory assumptions: change in the functional response.
  - Logistical assumptions: use of deterministic ODEs and continuous time with no space (we'll see all that in more depth in next section).

## What model formalism? (no maths shown here)

### Deterministic – stochastic processes

* **What does stochasticity come from?** 
  - Environmental (eg: perturbations): does not scale with population size and can vary in space and time.
  - Demographic (eg: more or less offsprings, trait variability that will impact biological rates): scales with population size. 
* **When should we account for it** (illustration with pop logistic growth (no math) with addition or not of gaussian noise)
  - for questions related to output variability
  - when output distribution is skewed and mean no more a good predictor (small populations) => when stochastic processes are dominant (ecological or genetical drifts, Allee effect, large demographic stochasticity)
* **When can we use deterministic**: when processes can be summarised with averaged parameters (large populations)
* NB: mention of master equations (deterministic equation summarizing stochastic model) 

### Time: discrete – continuous 

How do the processes are structured along time?  when using discrete vs continuous time 

* **What are the underlying assumptions?**
  - *Discrete-time*: 
    - Fixed generation times
    - Synchronization of processes (e.g., seasonal forcing)
    - Sequential processes with specific order (e.g., complex life-cycle) => more transparent 
  - *Continuous time*: 
    - Events can happen at any time
    - Generation overlap
* **Illustration with logistic pop growth** in continuous and discrete time:
  - discrete-time with small ∆t boils down to continuous
  - discrete time have richer dynamics due to a lag
* **Appropriate systems / questions**
  - discrete-time:
    - questions related to phenology
    - seasonal dynamics and synchronized generations
    - complex life cycled
  - continuous-time:
    - generation overlap
    - when processes can happen at any time
* Both can be either stochastic or deterministic

### Accounting for space 
* **None vs spatial**: 
  - TIB, example of space implicit; In none spatial models, space is assumed homogenous and the scale can be integrated within the units (example: pop density in ind/km2)
  - When does the question need it explicit space? 
    - When spatial processes impact local dynamics of interest
    - if we are interested in patterns emerging at higher spatial scale from local processes
* **Topology versus Distances explicit**: does geographical positions  matter or only topology?
  - Topology (metapop model): space is represented in the structure of space and strength of connectivity. Adapted to study fragmented landscapes or problems related to structure of connectivity
  - Explicit: distances are explicit; in 1 (Fisher KPP) or two dimensions (grid): adapted to represents fronts, gradients, edge effects or emerging spatial patterns.
* **Discrete within the distance explicit case**: grids vs Continuous (PDE):
  - *Continuous* adapted to model processes occurring progressing locally (diffusion) but not processes with large distance / heterogeneous effects
  - *Grids* with small mesh size can boil down to continuous (sometimes, discretisation is used for a technical approximation of continuous space)  => question of technical relevance.
  - As continuous can be more difficult (math) this can prevent its use (mention of lab inheritance)

  

## What technical choices?

### Analytical – Numerical

What do we gain in making models simple? => Principle of parcimony

* **Analytical solution**: we know the state of the model at any time point given $\frac{dN}{dt} \rightarrow f(t)$
* **Tractable model**: We are able to express equilibrium with parameters and study local stability $\frac{dN}{dt} \rightarrow N^*=f(parameters)$ => we can say generalities about the long term dynamics
* **Intractable models**: 
  - we are obliged to do simulations using numerical algorithm or integration to approximate the dynamics step by step.
  - the results depend on the parameter values and initial conditions.
  - The dimension of the parameter space to explore is exponentially linked to the number of parameters
  - Simulations allow to investigate transient dynamics

### Rules versus Maths

* **What is an agent-based model?** Algorithm which represent processes with a series of rules applied to each agent at each time step using proba: example with birth, death, interactions
* **What is the alternative**: Math => equations: example of ODE, SDE, DE, PDE: what does it mean?
  - ODE: change over time of our variable of interest (SDE; with some stochasticity) => give
  - DE: State at the next time step
  - PDE: Change over time and space
  - Overall idea: Balance between inputs and outputs. See with exponential model?

* **Advantages / Disadvantages**:
  - *ABM*: 
    + The dynamics emerge from elemental processes
    + Simpler to build from empirical knowledge
    + But high computation consumption
    + Rarely tractable
  - *Equations*
    + Use of math and approximation for simplifications
    + Have large analysis power for extreme cases
    + Fast computation
    + Easier to fit to data 
    + The relations between variables are imposed: processes synthetized
* **To what questions / system/ conditions is it most adapted?**
  - *ABM*: 
    + When stochastic processes are dominant (ex: conservation of small pops) 
    + When there are not too many parameters
    + For some questions where processes are difficult to synthesise (behaviour)
    + When there is no math skills around!
  - *Equations*:
    + Whenever processes can be synthesised with average parameters
    + => Large populations
* Such choices are often an inheritance from lab practices and habits.

## Some classical models used in ecology and seen next days 

* Systems of differential equations, no space : 
  - Verhulst (logistic growth), Lotka-Volterra predator-prey (Day 2)
  - Lotka-Volterra, food web niche model (Day 4)
* Spatial systems of differential equations: TIB, Levins’ occupancy (Day3)
* Spatial IBM: Neutral model of biodiversity from Hubbel (Day 3)

A Rmarkdown file is available to provide the code of the illustrations of  section 2.2


#  How to build a model? (Isa)
(1h)  Lecture with interactivity  (and Rmarkdown file for some parts)

* I will present different pictures of different plant-herbivore systems with associated research questions and ask how they will sketch the system (on board) to do a mode.
* I will ask then which formalism they would choose and why (in terms of determinism, time, space) [and we can try writing rules or equations?]
* Then we will focus on a question for which the Rosenzweig-MacArthur model (ODE)  is appropriate, I will write the model on board and ask all the assumptions they could see that are behind the model formulation. We will discuss in which regards they are appropriate for our question. 
* Then we will study line by line a code in R to explain how this could be implemented in R, while also explaining the principle of numerical integration.
* I will let them 5-10min  to run the model and plot some dynamics and ask them how they would use it to answer the question (to introduce the next section).


# How to analyze a model? (Isa)

(1h) Interactive lecture + based on a script for some parts

(Rmarkdown file with the example of Rosenzweig-MacArthur model)

* Analyze the behavior of the model long-term (Box 3 Grainger et al AmNat)
  - Local stability (when possible for long term dynamics) (script)
    + Calculating equilibria
    + What are the Jacobian matrix and eigenvalues (meaning but no math derivation) ?
    + How to interpret the eigenvalues to infer the long term behavior type
* Isoclines / graphical 
 *Phase plane (script)
* Bifurcation diagrams (long term dynamics) (script)
* When results depend on initial conditions => finding all the equilibria of intractable ODE systems (loop to run large series of initial conditions, function searchZeros of package nleqslv)

* Model Usages to answer the question (will try to show some concrete examples of questions to answer with these usages for the model of section 3a)
  - Parameter variation (see bifurcation diagrams + example DeMaz Grazing-Optimization)
  - Comparison with a null model to assess the impact of a process (with or without the process, with different formulation of the process. Example with functional responses (script)
  - Generation of synthetic data on which to run in silico experiments.     - Example of food webs under different regimes of perturbations.

* Parameter exploration and robustness of conclusions
  - Exhaustivity is possible	(tractable)
  - We know the values of parameters from empirical data => allows us to fix or restrain the range of some parameters.
  - Sensitivity analysis: effect size when varying 10% each parameter + look if conclusions are modified if the most sensitive parameters are varied.









<!--chapter:end:03-what-models.Rmd-->

# Temporal data (Day 2)

## Introduction

## Conclusion

<!--chapter:end:04-Temp-data.Rmd-->

# Spatial data (Day 3)

## Introduction
Comapre to Day 2, we will increase scale to consider 

Definition of a community and examples

The typical spatial dataset: a community matrix (sites*species), with either presence/absence, or detection/non detection, or abundance, or presence only, data
We might also have additional data: species traits, environmental variables, phylogenetic relationships

The typical questions we want to address

## Theoretical frameworks toattack these issues
We present three general frameworks used in this context:
-- The habitat filtering approach
-- The metacommunity framework
-- Vellend's community ecology theory framework
We list strengths/advantages for each framework
We point to this [cool website](https://reflectionsonpaperspast.wordpress.com/) for reflections from Vellend and Leibold on the fundamental publications introducing frameworks (2) and (3)

## DATA-DRIVEN (STATISTICAL) APPROACHES

### Community level methods
Unconstrained do not directly use environmental variables: a posteriori associations (PCA...)
Constrained methods simultaneously consider species composition and environmental descriptors (RDA...)
Short focus on Cottenie's approach of partitioning variance (partialRDA + PCNM) with examples from mollusk communities in the French Antilles
### Species level methods
Permutation approaches, the Cscore, etc.. ==> make a connection with Day 4 here (networks and permutations)
We mention the fact that even without interactions, several processes may yield spurious species associations (e.g. Calcagno et al. 2022)
Species Distribution Models
Occupancy models: their hallmark is to add a detection layer (probability to detect a species when it is present). We will talk more about this in the next section (mechanistic approaches)

## PROCESS-DRIVEN (MECHANISTIC) APPROACHES

### Patch-occupancy models

We start by introducing the two simplest and historically important models! Levins (1969) and McArthur & Wilson (1967) models.
We highlight the mathematical connection between Levins' model and the logistic model seen on the previous Day
There is an intrinsic covariation of r and K in this formulation!
We compute the equilibrium occupancies in the two models
We introduce the MW model of island biogeography: why are the curves non linear? We make them reflect on this, and ton the role of species trait variation in (c,e)
We present the example of fitting this model to island presence/absence data: data from Manne et al. (1998) J. Anim. Ecol.

We then discuss possible extensions:
-- explicit space (IFM models from Hanski and followers)
-- competition/colonization trade-offs, spatial networks, trophic webs...

### Models with explicit abundance dynamics in patches
These models quickly become complicated and need to be simulated.
One exception is Hubbel's neutral model, which has a dynamics for species abundances in patches and remains reasonably tractable
One example of simulation framework for metacommunities is Thompson et al. (2020) Ecology Letters. This is the one we will use to simulate data in the afternoon practical.

## Conclusion

<!--chapter:end:05-Spatial-data.Rmd-->

# Interaction networks, food webs, and complexity in ecology (Day 4)

## Preamble of the script used during the course

```{r}
library(cheddar)
library(igraph)
library(vegan)
library(sbm)
library(alluvial)
library(faux)
library(devtools) 
#Sys.unsetenv("GITHUB_PAT")
#install_github("FMestre1/fw_package")
library(FWebs)
library(matlib)
library(calculus)
library(pracma)
source('functions_network.R')

####In FWebs, there is a large list of food webs called mg1
sapply(1:length(mg1[[1]]),function(x) dim(as.matrix(mg1[[1]][[x]]))[1])

####Extract data from web 1,223
mat<-as.matrix(mg1[[1]][[223]])
plotMyMatrix(mat)
net.mat<-graph_from_adjacency_matrix(mat,mode="directed")
```

TO DO: change the way FWebs is used because it does not compile on Mac and Linux...

## Networks and associated definitions

### What is a network?
Networks (or graphs) are sets of nodes linked by edges.
Edges represent pairwise relationships between nodes.
Edges can be directed or not, weighted or binary (0/1).

### Networks in ecology
The main types of networks that you can encounter in ecology:
- the most common: interaction networks (edges are interactions, nodes are species)
- less common: spatial networks (edges are spatial connections, nodes are populations on a map)
- the rarest: assembly networks (edges are transitions between community compositions, nodes are community compositions)

### Ecological interaction networks
Since biotic interactions are very diverse, many different types of interaction networks can be built (mutualistic, antagonistic, etc.).
The type of data that can be obtained on networks is manifold:
- interaction frequency, especially from field observations (e.g. how many times a pollinator species is observed on a particular plant species);
- co-abundances or co-occurrences of the species pairs;
- interaction potentials, mostly obtained from experiments (e.g. arena experiments in which species are put together pairwise to observe interactions)

### A little history
The study of ecological interaction networks begun as early as the 50's with some landmark papers like Hutchinson's (Santa Rosalia, etc.) or Paine's series of keystone predation papers. The book of Allee is remarkable as it contains some of the earliest diagrams of food webs.

### Types of network
#### Unipartite networks
One large class of ecological networks is unipartite networks, i.e. messy ones in which interactions can occur between any two species. E.g. the Benguela food web of Yodzis (1998).
The function sample_gnp can generate random unipartite networks using the Erdos-Reyi model, which assumes that any interaction has the same probability to exist.
The plot function of igraph makes basic plots of networks, the layout option helps see things a wee bit more clearly.

```{r}
net<-sample_gnp(30,0.2,directed = FALSE)
plot(net)
net<-sample_gnp(30,0.2, directed =TRUE)
plot(net)

plot(net.mat,layout=layout_with_mds)
plot(net.mat,layout=layout_as_tree)
```

#### Multipartite networks
Another large class of networks is the one consisting of multipartite networks, i.e. networks in which nodes can be partitioned among different levels and nodes only interact among different levels and never within the same level.
Each species/node then has a "role" which defines its "level".
The most common type of multipartite network encountered in ecology is bipartite network (only two levels) such as plant-pollinator networks.
The function sample_bipartite generates a random bipartite network using the Erdos-Renyi model.

```{r}
net<-sample_bipartite(20,20,"gnp",0.1)
plot(net,layout=layout_as_bipartite)
```

## Representation of networks
### Binary networks
Having networks as collections of nodes and edges can be insightful, but to make computations on networks, one needs to express them using useful mathematical object. The easiest way is to express a network as a matrix called the adjacency matrix. The element $a_{ij}$ of an adjacency matrix is equal to 1 iff there exists an interaction between species $i$ and $j$. This also works for directed networks.
The function sample_grg given here generates a random graph based on a random geometric graph process -- it assumes that nodes have coordinates in 2D within a 1 x 1 square and interactions exist when the distance between two nodes is less than a given threshold (here, 0.2).
The function as_adjacency_matrix yields the adjacency matrix of the network.

```{r}
net<-sample_grg(30, 0.2)
as_adjacency_matrix(net)
```

### Weighted networks
When a network is weighted, the weighted adjacency matrix represents not only the existing/absent interactions, but also how strong these interactions are.
Here, we create random weights for the edges of network "net" using a Poisson distribution of mean 2.
Adding "[,]" after the name of network also yields the adjacency matrix.

```{r}
E(net)$weight<-rpois(length(E(net)),2)
net[,]
```

### Network matrices
Quite naturally, if a network is undirected, its adjacency matrix will be symmetric. When a network is directed, it will not necessarily be so.
One tricky item to keep in mind: sometimes food webs can be represented not using an adjacency matrix, but rather using an "energy flux" matrix which represents interactions as an antisymmetric matrix of +1's and -1's.
Adjacency matrices can be binary or real-valued (if they represent interaction weights).
A source of confusion can be the difference between adjacency and incidence matrices.

### Matrices for multipartite networks
In bipartite networks, the diagonal blocks of the adjacency matrix are full of 0 only. It is thus more useful to only look at the (rectangle) non-zero off-diagonal blocks, which are called the incidence matrix in ecology (or bi-adjacency matrix in other disciplines).
The function as_biadjacency_matrix of igraph extracts this matrix from a bipartite network.

```{r}
net<-sample_bipartite(4,4,"gnm",m=8)
net[,]
as_biadjacency_matrix(net)
```

### Degrees
A basic element of representation of networks is the degree of a node.
The degree of a node is the number of interactions it shares with other nodes.
For undirected networks, this definition is straightforward and corresponds to row- or column-sums of the binary adjacency matrix.
For directed networks, one actually needs to define the in-degree and out-degree of each node, which correspond respectively to the number of incoming and outgoing edges.
The function degree of igraph computes all of these using the argument called mode.

```{r}
degree(net)
degree(net,mode="in")
degree(net,mode="out")
```

The degree distribution corresponds to the realized empirical distribution of degrees in the network. It can be obtained using a variety of functions, including degree_distribution in igraph.

```{r}
hist(degree(net.mat),breaks=0:max(degree(net.mat)))
plot(degree_distribution(net.mat, cumulative = TRUE))
```

### Connectance
Connectance is an important notion. It is defined as the proportion of possible interactions that really exist. There are plenty of ways to implement its computation (e.g. using mean), but it is important to correct these according to the impossible interactions (e.g. correcting for self-links or edges within levels in bipartite networks).

```{r}
conn<-mean(as.matrix(net.mat[,]))
conn
```

## Crash course: matrices, eigenvalues, Jacobian matrix

### Crash course: matrices
Matrices are mathematical objects that represent tables, i.e. have rows and columns. The element at row $i$ and column $j$ of matrix $M$ is often noted $m_{ij}$.
Matrices and vectors follow a strange multiplication operation, noted with %*% in R (rather than * which encodes a more plastic multiplication operation).

```{r}
m<-matrix(rbinom(9,1,0.5),nrow=3)
x<-rnorm(3)
m*x
m%*%x
```

### Crash course: matrix eigenvalues
All matrices are associated with a set of values called "eigenvalues" which represent the "multiplicators" of the matrix. Each of these eigenvalues $\lambda$ is associated to eigenvectors $x$ such that multiplying $x$ by the matrix $M$ results in only multiplying $x$ by $\lambda$:
$$M.x = \lambda x$$

The collection of eigenvalues of $M$ is called its spectrum. The spectrum is always finite and has at most a number of distinct elements equal to the smallest dimension of $M$.

The function eigen yields the eigensystem (eigenvalues and eigenvectors) of a matrix.

```{r}
eigen(m)
```

### Crash course: Jacobian matrices
A set of ordinary differential equations (ODEs), such as those given by Lotka-Volterra equations, can be written in compact form as:

$$\frac{d\overrightarrow{x}}{dt} = \overrightarrow{F}\left(\overrightarrow{x} \right)$$

Let us assume that there exists an equilibrium $\overrightarrow{x}^*$ for this system. If we want to assess whether this equilibrium is stable, we need to linearize the dynamics around the equilibrium:

$$\frac{d\left( \overrightarrow{x}-\overrightarrow{x}^*\right)}{dt} \approx J\left(\overrightarrow{x}^*\right).\left( \overrightarrow{x}-\overrightarrow{x}^*\right)$$

The matrix $J\left(\overrightarrow{x}^*\right)$ is called the Jacobian matrix of the dynamical system at the equilibirum $\overrightarrow{x}^*$
The expression for eh Jacobian matrix is obtained using the partial derivatives of the different components of $\overrightarrow{F}$:
$$J\left(\overrightarrow{x}^*\right) = \left.\left(\partial F_i/\partial x_j\right)\right\rvert_{x=x^*}$$

Formal computations of Jacobian matrices can be done with R using the package calculus:

```{r}
#jacobian(c("r*n1*(1-n1/k)-a*n1*n2/(1+h*n1)","b*n1*n2/(1+h*n1)-d*n2"), var = c("n1", "n2"))
```

## Theories and notable results

### Lotka-Volterra and networks
This is the classic Lotka-Volterra system, given for a dimensionless system (i.e. the coefficient of self-feedback are all set to 1):
$$\frac{dx_i}{dt} = x_i \left[ r_i - x_i + \sum_j a_{ij}x_j\right]$$
This system of ODEs can be seen as describing the dynamics of a network of species, with matrix $A$ as its "weighted" adjacency matrix.

### May's stability result
In the 70's, Sir Robert May published a very important mathematical ecology result which shook some of the beliefs ecologists had at that time.
The idea is to assume that a community consists of $S$ species and that species abundances of these species are fixed at an equilibrium value $\overrightarrow{x}^*$. May's approach asks the question: what should happen if elements of the Jacobian matrix were randomly drawn according to a simple distribution? More precisely, May assumed that diagonal elements of the Jacobian were equal (or similar) and negative, while the off-diagonal elements had a probability $c$ (the connectance) of being non-zero, and if not equal to zero, followed a normal distribution of mean 0 and standard deviation $\sigma$.
The result of May thus stipulates that the equilibrium can only be stable when
$$\begin{equation} 
\sigma \sqrt{cS} < -\overline{J_{ii}} 
\end{equation}$$

This result can be explained geometrically (based on the circular law): for a random matrix with this particular distribution of off-diagonal elements, the distribution of eigenvalues will be enclosed in a disk of center $\left(-\overline{J_{ii}},0\right)$ and radius $\sigma \sqrt{cS}$ in the complex plane. For all eigenvalues to have negative real parts, the radius of this disk should be less than the distance between (0,0) and its center.

```{r,echo=FALSE}
knitr::include_graphics("fig06Networks/May.jpg")
```

```{r}
m<-matrix(rnorm(10^6),nrow=10^3)
plot(eigen(m)$values,asp=1)
```
This result can be interpreted as follows: for a given equilibrium to be stable, it should have low diversity and/or low connectance and/or a low variability of non-diagonal Jacobian elements (i.e. feedbacks among species abundances) and/or highly negative diagonal Jacobian elements (i.e. strong negative feedbacks of species upon themselves).

Intuitively, this does not seem to conform to reality: we see species-rich systems in which species interact relatively strongly and not too sparsely, without intense self-regulation...
So, some of the assumptions must be false and we can look for changes to these assumptions:
- dynamics could be transient rather than at equilibrium
- interactions might be structured and not completely random (see Claire's talk)

### Feasibility vs. stability
One other strong argument contradicting May's result is the following: in many cases, it might actually be more difficult to obtain an equilibrium including all species (i.e. have feasibility) than for this equilibrium to be stable.
For instance, Bizeul & Najim proved that in a certain class of LV systems, feasibility is obtained only when 
$$\sigma \sqrt{2Slog(S)} < -\overline{J_{ii}}$$
where $\sigma$ is the standard deviation of the LV matrix (not the Jacobian matrix) and the connectance is merged into $\sigma^2$.

In the simulation paper of L. Stone, similar results were obtained -- here stability (on the left) is lost after feasibility (on the right) when increasing of a parameter ($\gamma$) which is proportional to our $\sigma$.

```{r,echo=FALSE}
knitr::include_graphics("fig06Networks/Stone.jpg")
```

### Food web invariants
In the 80-90's, one important goal of interaction network research was to find invariant properties, i.e. statistics or metrics that did not vary among food webs. With this goal in mind, two models were proposed.
First, the cascade model of Cohen & Briand proposed that predators only eat a rather fixed number of prey species. This corresponds to predators eating prey that are are smaller than them with probability $c/S$ ($c$ being a fixed parameter), so that connectance decreases as $1/S$.
```{r}
m<-cascade_matrix(10,20)
sum(m)/(dim(m)[1]*(dim(m)[1]-1))

m<-cascade_matrix(10,200)
sum(m)/(dim(m)[1]*(dim(m)[1]-1))
```

The second model that was developed to find invariants is the niche model of Williams and Martinez. In this model, all species are supposed to be defined by their size and to eat all species with size contained within an interval centred around a size lower than the predator's.
With the right parameter values, one can use this model to generate a food web that has a given connectance, whatever the number of species in the food web.

```{r}
niche<-niche_matrix(0.2,100)
m<-niche$matrix
sum(m)/(dim(m)[1]^2)

niche<-niche_matrix(0.2,200)
m<-niche$matrix
sum(m)/(dim(m)[1]^2)
```
### Generating the niche model
Exercise: generate 100 virtual food webs similar to the one described by mat

```{r}
niches<-lapply(1:100,function(x) niche_matrix(conn,dim(mat)[1]))
ms<-lapply(1:100,function(x) niches[[x]]$matrix)
```

### Robustness to secondary extinctions
The general idea is to assume that when a species is removed from a network, the species interacting with it could go extinct (secondary extinction).
Robustness analysis then consists in assessing how many species are lost when R species are removed.
The most used indicator of robustness is the R50 which is the percentage of species on needs to remove from a web to lose 50% of its species.

```{r,echo=FALSE}
knitr::include_graphics("fig06Networks/Dunne1.jpg")
```

In the paper of J. Dunne and collaborators, they used examples based on a variety of datasets. The basic rule then is that a species go extinct when it loses all its prey items (except itself if it is cannibalistic).
One can obtain different scenarios based on how removed species are chosen (intentionality: how strongly the species' degree changes its probability of being drawn randomly as the next one to be removed).

```{r,echo=FALSE}
knitr::include_graphics("fig06Networks/Dunne2.jpg")
```

```{r}
net<-graph_from_adjacency_matrix(m,mode="directed")
i_index <- seq(from = 0, to = 1, by =0.1)
i_index <- head(i_index,-1)
prob_exp<-exponent.removal(net, i_index)
V(net)$name<-1:200
iterate(fw_to_attack=net, prob_exp, alpha1=50, iter=10, i_index, plot = TRUE)
```

## A quick primer on network statistics

### Degree distributions
Basically, one can do three things with degree distributions:
- compute them empirically
- generate networks similar to one that is observed using its degree sequence
- compare the degree distribution to benchmark distributions of integers (Poisson, power law, ...) or of degrees of well-known random networks (Erdos-Renyi, random geometric graph, Watts-Strogatz...)

```{r,echo=FALSE}
knitr::include_graphics("fig06Networks/Dunne3.jpg")
```

```{r}
net<-graph_from_adjacency_matrix(m)
hist(degree(net),breaks=0:max(degree(net)))
plot(degree_distribution(net, cumulative = TRUE))

net<-sample_degseq(degree(net),method = "vl")

mean(degree(net))
ks.test(degree(net),"pbinom",length(V(net)),mean(degree(net))/length(V(net)))
```

Power laws are among the most popular distributions of degrees discussed in the literature. However, a question worth asking is whether power laws are useful in ecology, i.e. whether it is actually possible to ascertain power-lawed degrees in ecological networks.
As Stumpf & Porter (2012) clearly articulated, this would require to have degrees ranging at the very least between 1 and 100  (2 orders of magnitude), which is quite unheard of.

### Null models / randomizations

Statistics on networks are hard to test because everything depends on everything else in a network...

So to test for extraordinary patterns in a network (or between them), one can resort to two strategies:
- either to define a probabilistic model to build a null hypothesis (e.g. using the expected degree distribution)
- or to define a null model based on some randomization scheme

One of the most useful and simplest such scheme is the configuration model. Under this null model, edges are randomly reattached to other nodes such that all nodes keep their degrees. This behaves effectively as if all nodes were attached to half-edges which were randomly paired until the resulting network yields no self-link and no double link.

```{r,echo=FALSE}
knitr::include_graphics("fig06Networks/configuration.jpg")
```

For bipartite binary networks, there is an algorithm (curveball) which yields the configuration model quickly and efficiently.

```{r}
net<-sample_bipartite(50,50,"gnp",0.1)
sample.bip.config<-simulate(nullmodel(as_incidence_matrix(net),"curveball"),nsim=1000)
dim(sample.bip.config)
```

For unipartite binary networks, the task is slightly harder, but doable using sample_degseq.

```{r}
n<-200 #1000
net<-sample_gnp(n,0.2, directed = FALSE)
sample.config.undirected<-lapply(1:100,function(x) sample_degseq(degree(net), method = "vl"))
length(sample.config.undirected)
```

For directed networks, it is even worse because we have no indication that the random matrices are all explored and represented fairly among the realizations of the algorithm.

```{r}
net<-sample_gnp(n,0.2, directed = TRUE)
sample.config.directed<-lapply(1:100,function(x) sample_degseq(degree(net,mode="out"), degree(net,mode="in"), method = "simple.no.multiple"))
length(sample.config.directed)
```

Finally, for any weighted network (bipartite or unipartite), the task is even more difficult.

### Network structure
Ecological interaction networks can have non-random structures. In ecology, we tend to focus on two such structures: modularity (on the left) and nestedness (on the right).

```{r,echo=FALSE}
knitr::include_graphics("fig06Networks/Lewinsohn.jpg")
```

In the following we will more strongly focus on modularity than on nestedness, for reasons that will be explained later.

### Modularity
Modularity, as defined by Newman, is given by the following formula:
$$\begin{equation}
Q = \frac{1}{A} \sum_{i,j} \left[ a_{ij} - \frac{d_i d_j}{A}\right]\delta_{ij}
\end{equation}$$
where $A$ is the total number of connections in the network (i.e. $A = \sum_i \sum_j a_{ij}$), $d_i$ is the degree of species $i$ (i.e. $d_i = \sum_j a_{ij}$) and the $\delta_{ij}$ are dummy variables indicating whether species/nodes $i$ and $j$ are assumed to belong to the same module/community/cluster. 
In layman's terms, modularity is a quantity that measures how ``intense'' interactions are within modules, and thus by contrast how little interaction there is between nodes belonging to different modules.
The principle of modularity search is to look for the partitioning of nodes into communities/modules that maximizes the associated modularity score. 
The problem of modularity search is quite complex (NP-complete problem) and thus can be solved with different algorithms that have their pros and cons.

Modularity, in its classic form, only works for undirected networks.
Three algorithms that are often used in ecology are the edge-betweenness algorithm (EB), the leading-eigenvector one (LE) and the Louvain algorithm (ML).

```{r}
net<-sample_gnp(100,0.2, directed = FALSE)
EB.mod<-cluster_edge_betweenness(net)
LE.mod<-cluster_leading_eigen(net)
ML.mod<-cluster_louvain(net)

plot(EB.mod,net,layout = layout_with_mds)
plot(LE.mod,net,layout = layout_with_mds)
plot(ML.mod,net,layout = layout_with_mds)
```

Again, modularity is not adapted to directed networks, but there ways to circumvent this issue:
- by making the network/adjacency matrix symmetric
- through the use of another definition of modularity (e.g. the infomap definition of Rosvall and Bergström)

#### Modularity of the empirical food web
As mentioned before, it is highly recommended to check whether the obtained modularity is ``expected'' or extraordinary using a null model. Here is an example using the empirical food web.

```{r}
modul<-cluster_louvain(graph_from_adjacency_matrix(mat,mode="undirected"))
moduls<-lapply(1:100,function(x) cluster_louvain(graph_from_adjacency_matrix(ms[[x]],mode="undirected"))$modularity)
plot(density(unlist(moduls)))
modul.ecdf<-ecdf(unlist(moduls))
1-modul.ecdf(modul$modularity[2])
```

#### Which algorithm?
Because there are tons of algorithms already developed to find clusters/communities/modules in networks, it is easy to get lost... Yet two papers (at least) have made our life easier by comparing all these algorithms, bot for unipartite networks (Yang et al. 2016) and bipartite networks (Leger et al. 2015).

### Block models
Another possibility when looking for clusters of species that interact more among themselves is to look for blocks of nodes/species that ``behave'' similarly. This can be achieved using stochastic block models (SBM), also sometimes called latent block models (LBM) when dealing with bipartite networks.

Block models work by optimizing the fit of a model (in the classic statistical sense) which assigns probability of membership to groups and probability of interaction between nodes of known groups.
Block models can be fitted on different kinds of adjacency matrices following different distributions (e.g. Bernoulli for binary adjacency matrices, Poisson for counts, etc.).
The inference of a block model yields one parameter per block (the ``expected connection'' within the block) and the proportions of nodes belonging to each group.

```{r}
sbmnet <- sampleSimpleSBM(100, c(.5, .25, .25), list(mean = diag(.4, 3) + 0.05), model = 'bernoulli')
head(sbmnet$networkData)
net.SBM <- estimateSimpleSBM(as.matrix(sbmnet$networkData))
plot(net.SBM, 'expected')
plot(net.SBM, 'data')
```

### Modularity and blocks
It is possible to compare the partitionings of nodes obtained through modularity search and SBMs, for instance comparing the result of Louvain algorithm with a simple SBM on the network given by mat.

The alluvial plot helps visualize the correspondence between these groups.
```{r}
m.SBM <- estimateSimpleSBM(mat)
modul<-cluster_louvain(graph_from_adjacency_matrix(mat,mode="undirected"))
make_alluvial_2(m.SBM$memberships,modul$membership,"Blocks","Modules")
```

It is also possible to look at whether these groupings have anything to do with trophic level (sensus MacKay et al.).
```{r}
count_components(net.mat)
net.comp<-components(net.mat)
tl.1<-trophic_levels(largest_component(net.mat))
plot(largest_component(net.mat),layout = layout_as_food_web)
plot(tl.1~as.factor(m.SBM$indMemberships[which(net.comp$membership==1),]%*%(1:3)),xlab="SBM group",ylab="Trophic level")
plot(tl.1~as.factor(modul$membership[which(net.comp$membership==1)]),xlab="module",ylab="Trophic level")
```

As we can see, SBM-derived groups seem to be better at capturing trophic levels.

### Spectral clustering
One drawback of SBM is that they can be slow to converge, especially when the dataset is big. A convenient replacement algorithm when the number of groups to look for is known (or can be easily guessed) is spectral clustering.
The idea of spectral clustering is to make use of the Laplacian matrix $L$ of the graph, given by:
$$L = D - A$$
where $D$ is the diagonal matrix of nodes' degrees and $A$ is the adjacency matrix.

The Laplacian matrix has interesting properties, the best of all being that the number of zeros among its eigenvalues yield the number of components of the graph (one 0 means there is a single connected component, two zeros means the network is split in two disconnected components, etc.). The eigenvectors associated the zero eigenvalues can inform on the membership of nodes to the different components.

The heuristics of the spectral clustering is to use the eigenvectors associated with the set of small non-zero eigenvalues of $L$ to deduce the ``almost components'' of the graph. To do so, one chooses a number of no-zero eigenvalues to keep and then the algorithm uses the associated eigenvector to find the groups through a K-means algorithm on the space obtained with these eigenvectors.

Here is an example of spectral clustering on the previous block-model graph.

```{r}
SC<-spectral_clustering(graph_from_adjacency_matrix(sbmnet$networkData),3)
plotMyMatrix(sbmnet$networkData,clustering=list("row"=SC,"col"=SC))
```

### Nestedness
The idea of nestedness is to measure the tendency for specialists to only interact with a subsample of the interactors of generalists. In other words, a network would be said ``non-nested'' only when specialists would interact more likely with specialists than with generalists.

Several indices have been developed over the years. Navigating their complexity is the purpose of the synthesis written by Podani and Schmera (2012).

I will not delve too long on the topic of nestedness because recent research seem to indicate that the concept is not really fruitful for a variety of reasons:
- it seems to be very strongly linked to connectance
- the diversity of nestedness indices tend to obfuscate its meaning (results reported show more about the index than about the concept)
- these different indices can yield very different results when applied to the same dataset
- in the classic case of power law distributions of degrees, the nestedness scores tend to be completely detrmined by the exponent of the power law
- finally, when putting the common datasets in the right matrix ensemble (i.e. using correct null models to sample similar matrices), reported nestedness values seem to be completely expected.

### Last words
- ecological interaction networks can be used to study community or ecosystem properties
- random network models can help gauge generalities and specificities of networks
- all network patterns do not have a mechanistic model-based explanation yet, but some do (like the link between feasibility, stability, or robustness and connectance)
- interaction network data of different types lend themselves to different analyses (e.g. modularity for binary networks, value adjacency matrix with signed interactions for May-like analyses)

## Literature

Astegiano, J., Guimarães Jr, P. R., Cheptou, P.-O., Vidal, M. M., Mandai, C. Y., Ashworth, L. & Massol, F. (2015) Persistence of plants and pollinators in the face of habitat loss: Insights from trait-based metacommunity models. Advances in Ecological Research - vol. 53 (eds G. Woodward & D. A. Bohan), pp. 201-257. Academic Press.

Bizeul, P. & Najim, J. (2021) Positive solutions for large random linear systems. Proceedings of the American Mathematical Society, 149, 2333-2348.

Clauset, A., Shalizi, C. R. & Newman, M. E. J. (2009) Power-law distributions in empirical data. SIAM Review, 51, 661-703.

Dunne, J. A., Williams, R. J. & Martinez, N. D. (2002) Food-web structure and network theory: The role of connectance and size. Proceedings of the National Academy of Sciences, 99, 12917-12922.

Dunne, J. A., Williams, R. J. & Martinez, N. D. (2002) Network structure and biodiversity loss in food webs: robustness increases with connectance. Ecology Letters, 5, 558-567.

James, A., Pitchford, J. W. & Plank, M. J. (2012) Disentangling nestedness from models of ecological complexity. Nature, 487, 227-230.

Leger, J.-B., Daudin, J.-J. & Vacher, C. (2015) Clustering methods differ in their ability to detect patterns in ecological networks. Methods in Ecology and Evolution, 6, 474-481.

Lewinsohn, T. M., Prado, P. I., Jordano, P., Bascompte, J. & Olesen, J. M. (2006) Structure in plant-animal interaction assemblages. Oikos, 113, 174-184.

May, R. M. (1972) Will a large complex system be stable? Nature, 238, 413-414.

Newman, M. E. J. (2006) Modularity and community structure in networks. Proceedings of the National Academy of Sciences, 103, 8577-8582.

Pocock, M. J. O., Evans, D. M. & Memmott, J. (2012) The robustness and restoration of a network of ecological networks. Science, 335, 973-977.

Payrató-Borràs, C., Hernández, L. & Moreno, Y. (2019) Breaking the Spell of Nestedness: The Entropic Origin of Nestedness in Mutualistic Systems. Physical Review X, 9, 031024.

Podani, J. & Schmera, D. (2012) A comparative evaluation of pairwise nestedness measures. Ecography, 35, 889-900.

Stone, L. (2016) The Google matrix controls the stability of structured ecological and biological networks. Nature Communications, 7, 12857.

Strona, G., Nappo, D., Boccacci, F., Fattorini, S. & San-Miguel-Ayanz, J. (2014) A fast and unbiased procedure to randomize ecological binary matrices with fixed row and column totals. Nature Communications, 5.

Stumpf, M. P. H. & Porter, M. A. (2012) Critical truths about power laws. Science, 335, 665-666.

Thomas, M., Verzelen, N., Barbillon, P., Coomes, O. T., Caillon, S., McKey, D., Elias, M., Garine, E., Raimond, C., Dounias, E., Jarvis, D., Wencélius, J., Leclerc, C., Labeyrie, V., Cuong, P. H., Hue, N. T. N., Sthapit, B., Rana, R. B., Barnaud, A., Violon, C., Arias Reyes, L. M., Latournerie Moreno, L., De Santis, P. & Massol, F. (2015) A network-based method to detect patterns of local crop biodiversity: validation at the species and infra-species levels. Advances in Ecological Research, 53, 259-320.

Williams, R. J. & Martinez, N. D. (2000) Simple rules yield complex food webs. Nature, 404, 180-183.

Yang, Z., Algesheimer, R. & Tessone, C. J. (2016) A Comparative Analysis of Community Detection Algorithms on Artificial Networks. Scientific Reports, 6, 30750.

Yodzis, P. (1998) Local trophodynamics and the interaction of marine mammals and fisheries in the Benguela ecosystem. Journal of Animal Ecology, 67, 635-658.



<!--chapter:end:06-Networks.Rmd-->

